# Ollama Multi-LLM API

A FastAPI service for managing multiple specialized Ollama language models with monitoring, logging, and specialized endpoints.

## Features

- **Multi-Model Support**: Access to multiple specialized LLM models (LlamaSharp, CodeQwen, CodeLlama, Mistral, Phi-3, LLaVA)
- **Database Logging**: All interactions are logged to an Oracle database
- **Performance Monitoring**: CPU usage tracking during model inference
- **Specialized Endpoints**: Dedicated endpoints for each model type
- **Image Processing**: Support for multimodal models (LLaVA) with image inputs
- **Interaction History**: Query and retrieve past interactions

## Models Available

| Model       | Specialization                     | Endpoint       |
|-------------|------------------------------------|----------------|
| LlamaSharp  | Structured reasoning & analysis    | `/llamasharp`  |
| CodeQwen    | Fast code generation               | `/codeqwen`    |
| CodeLlama   | Code debugging & optimization       | `/codellama`   |
| Mistral     | General-purpose conversations      | `/mistral`     |
| Phi-3       | Efficient, concise responses       | `/phi3`        |
| LLaVA       | Multimodal (image + text) processing| `/llava`      |

## Installation

### Prerequisites
- Python 3.8+
- Ollama running locally or accessible via `OLLAMA_URL`
- Oracle Database (for logging)
- Required Python packages (see `requirements.txt`)

### Setup

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd <project-directory>
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Set environment variables:
   ```bash
   export OLLAMA_URL=http://localhost:11434  # Default if not set
   ```

4. Ensure your Oracle database has the required table structure:
   ```sql
   CREATE TABLE llm_interactions (
       id NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY PRIMARY KEY,
       model_name VARCHAR2(100) NOT NULL,
       system_prompt CLOB,
       user_prompt CLOB NOT NULL,
       response_text CLOB,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       response_time_ms NUMBER,
       success NUMBER(1) DEFAULT 1
   );

   CREATE TABLE cpu_usage_metrics (
       id NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY PRIMARY KEY,
       interaction_id NUMBER NOT NULL,
       model_name VARCHAR2(100) NOT NULL,
       user_prompt CLOB,
       response_time_ms NUMBER,
       cpu_usage NUMBER,
       timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       FOREIGN KEY (interaction_id) REFERENCES llm_interactions(id)
   );
   ```

## Usage

### Running the API
```bash
uvicorn apis.prompts:app --host 0.0.0.0 --port 8000
```

### Making Requests

#### Basic Text Generation
```bash
curl -X POST "http://localhost:8000/llamasharp" \
-H "Content-Type: application/json" \
-d '{"user_prompt": "Explain quantum computing in simple terms"}'
```

#### Image Processing with LLaVA
```bash
curl -X POST "http://localhost:8000/llava" \
-H "Content-Type: application/json" \
-d '{"user_prompt": "Image Path: /path/to/your/image.png\nWhat does this diagram show?"}'
```

#### Getting Interaction History
```bash
# Get all interactions (paginated)
curl "http://localhost:8000/interactions?limit=5&offset=0"

# Search by prompt content
curl "http://localhost:8000/interaction/quantum"

# Get specific interaction by ID
curl "http://localhost:8000/interactions/123"
```

### Response Format
All endpoints return JSON in this format:
```json
{
  "response": "The generated text response",
  "model": "model-name",
  "prompt_id": 12345,
  "response_time_ms": 1200.5
}
```

## API Endpoints

| Endpoint                     | Method | Description                                  |
|------------------------------|--------|----------------------------------------------|
| `/health`                    | GET    | Health check for API and dependencies        |
| `/models`                    | GET    | List available models and their descriptions|
| `/llamasharp`                | POST   | Generate response using LlamaSharp           |
| `/codeqwen`                  | POST   | Generate code using CodeQwen                 |
| `/codellama`                 | POST   | Debug/optimize code using CodeLlama          |
| `/mistral`                   | POST   | General conversation using Mistral           |
| `/phi3`                      | POST   | Get efficient responses using Phi-3          |
| `/llava`                     | POST   | Process images with LLaVA                    |
| `/interactions`              | GET    | List all interactions (paginated)           |
| `/interactions/{id}`         | GET    | Get specific interaction by ID              |
| `/interaction/{search_term}` | GET    | Search interactions by prompt content       |

## Configuration

Environment variables:
- `OLLAMA_URL`: Base URL for Ollama service (default: `http://localhost:11434`)

Database connection is hardcoded to:
- User: `SYS`
- Password: `oracle`
- DSN: `10.42.0.243:1521/FREE`

## Monitoring

The API automatically:
1. Logs all interactions to the database
2. Tracks CPU usage during model inference (sampled every 100ms)
3. Records response times for all requests

## Error Handling

The API returns appropriate HTTP status codes:
- `404` for unknown models or missing interactions
- `500` for server errors (with details in response)
- `400` for invalid requests

## Development

To modify or extend:
1. Add new models to `MODEL_CONFIGS` dictionary
2. Create new endpoints following the existing pattern
3. Extend database schema as needed for new features

## License

[Specify your license here]
