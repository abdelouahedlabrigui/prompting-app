from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
import oracledb
from datetime import datetime
from typing import Optional
import json
import logging
from contextlib import asynccontextmanager
import os

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Ollama configuration
OLLAMA_BASE_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")

# Connection pool
connection_pool = None

# Database schema - run this SQL to create the table
"""
CREATE TABLE llm_interactions (
    id NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY PRIMARY KEY,
    model_name VARCHAR2(100) NOT NULL,
    system_prompt CLOB,
    user_prompt CLOB NOT NULL,
    response_text CLOB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    response_time_ms NUMBER,
    success NUMBER(1) DEFAULT 1
);
"""

# Pydantic models
class PromptRequest(BaseModel):
    user_prompt: str
    system_prompt: Optional[str] = None

class LLMResponse(BaseModel):
    response: str
    model: str
    prompt_id: int
    response_time_ms: float

# Model configurations with specialized system prompts
MODEL_CONFIGS = {
    "llamasharp": {
        "name": "JorgeAtLLama/llamasharp:latest",
        "system_prompt": (
            "You are LlamaSharp, an AI optimized for reasoning and structured analysis. "
            "Always prioritize accuracy and clarity. Break problems into logical steps, "
            "explain reasoning only when it improves understanding, and avoid speculation. "
            "If unsure, state limitations clearly."
        ),
        "description": "Optimized for structured reasoning and factual accuracy"
    },
    "codeqwen": {
        "name": "codeqwen:latest", 
        "system_prompt": (
            "You are CodeQwen, an expert coding assistant. "
            "Prioritize generating correct, clean, and efficient code. "
            "Always provide runnable solutions with minimal explanation unless clarification is requested. "
            "Use best practices, error handling, and concise comments when useful. "
            "If the request is ambiguous, ask clarifying questions before coding."
        ),
        "description": "Specialized for fast, accurate code generation"
    },
    "codellama": {
        "name": "codellama:latest",
        "system_prompt": (
            "You are CodeLlama, an assistant focused on code understanding, debugging, and optimization. "
            "Analyze code precisely, identify errors, and propose improvements with working examples. "
            "Keep explanations short and technical. "
            "Avoid speculative answers and ensure suggestions are valid and testable."
        ),
        "description": "Expert in debugging, optimization, and code explanations"
    },
    "mistral": {
        "name": "mistral:latest",
        "system_prompt": (
            "You are Mistral, a versatile AI for general conversation, creativity, and problem-solving. "
            "Be accurate, context-aware, and engaging. "
            "Keep answers concise and relevant, avoiding unnecessary detail. "
            "If uncertain, provide best-effort reasoning but acknowledge limits."
        ),
        "description": "General-purpose assistant for diverse tasks"
    },
    "phi3": {
        "name": "phi3:latest",
        "system_prompt": (
            "You are Phi-3, an efficient assistant optimized for speed and precision. "
            "Provide concise, factual, and direct responses. "
            "Focus on accuracy over speculation. "
            "For coding, return minimal but correct solutions. "
            "When brevity risks confusion, add one clarifying sentence."
        ),
        "description": "Efficient assistant for quick, accurate responses"
    },
    "llava": {
        "name": "llava:7b",
        "system_prompt": (
            "You are an expert at solving mathematical problems and coding python methods. "
            "When you receive a mathematical exercise, from png file path, first analyze it carefully. "
            "Then, think step by step how to best approach the problem. "
            "Finally, write a python method to solve the problem and return the solution."
        ),
        "description": ""
    }
}



app = FastAPI(
    title="Ollama Multi-LLM API",
    description="FastAPI service for managing multiple specialized Ollama language models",
    version="1.0.0",
    # lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://10.42.0.243:4200",
        "http://10.0.0.243:4200",
        "http://localhost:4200",
        "http://10.42.0.243:3000",
        "http://10.0.0.243:3000",
        "http://localhost:3000"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
import asyncio
import psutil  # Import the psutil library
# Database operations
async def get_db_connection():
    """Get database connection from pool"""
    if not connection_pool:
        raise HTTPException(status_code=500, detail="Database connection pool not available")
    return oracledb.connect(user="SYS", password="oracle", dsn="10.42.0.243:1521/FREE", mode=oracledb.SYSDBA)

async def get_system_metrics():
    """
    Get CPU usage.
    Returns:
        cpu_usage (float): CPU usage percentage
    """
    cpu_usage = psutil.cpu_percent(interval=None)  # Get instantaneous CPU usage
    return cpu_usage

async def save_interaction(model_name: str, system_prompt: str, user_prompt: str, 
                         response_text: str, response_time_ms: float, success: bool = True):
    """Save LLM interaction to database"""
    try:
        connection = oracledb.connect(user="SYS", password="oracle", dsn="10.42.0.243:1521/FREE", mode=oracledb.SYSDBA)
        cursor = connection.cursor()
        
        sql = """
        INSERT INTO llm_interactions 
        (model_name, system_prompt, user_prompt, response_text, response_time_ms, success)
        VALUES (:model_name, :system_prompt, :user_prompt, :response_text, :response_time_ms, :success)
        RETURNING id INTO :interaction_id
        """
        
        interaction_id_var = cursor.var(int)
        cursor.execute(sql, {
            'model_name': model_name,
            'system_prompt': system_prompt,
            'user_prompt': user_prompt,
            'response_text': response_text,
            'response_time_ms': response_time_ms,
            'success': 1 if success else 0,
            'interaction_id': interaction_id_var
        })
        
        connection.commit()

        result = interaction_id_var.getvalue()[0]
        connection.close()
        return result
    except Exception as e:
        logger.error(f"Error saving interaction to database: {e}")
        raise HTTPException(status_code=500, detail="Failed to save interaction to database")

from config.model_options import ModelOptions
from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate


import os
import re
import base64
import asyncio
from io import BytesIO
from typing import Optional

from PIL import Image
from langchain_core.messages import HumanMessage, SystemMessage

async def call_ollama_model_for_image_to_text(model_name: str, system_prompt: Optional[str], user_prompt: str) -> str:
    """
    Call an Ollama model. If an image path is detected (line starting with 'Image Path:'), and the
    target model is LLaVA (or multimodal), the image will be read, encoded to base64 and sent
    as part of a HumanMessage content parts (image + text). Otherwise this makes a normal text call.
    """
    start_time = datetime.now()
    config = MODEL_CONFIGS.get(model_name)
    if not config:
        raise ValueError(f"Model '{model_name}' not found in MODEL_CONFIGS.")

    final_system_prompt = system_prompt or config.get("system_prompt", "")
    llm = ChatOllama(
        model=config["name"],
        num_keep=-1,
        seed=42,
        num_predict=-1,
        top_k=20,
        top_p=0.9,
        num_thread=2,
        num_gpu=-1,
    )

    # try to extract "Image Path: <path>" (captures spaces)
    m = re.search(r'Image Path:\s*(.+)$', user_prompt, flags=re.IGNORECASE | re.MULTILINE)
    is_multimodal_model = "llava" in config["name"].lower() or "llava" in model_name.lower()

    if m and is_multimodal_model:
        image_path = m.group(1).strip().strip('\'"')  # remove possible quotes
        # remove the Image Path line from the textual prompt
        text_prompt = re.sub(r'Image Path:\s*(.+)$', '', user_prompt, flags=re.IGNORECASE | re.MULTILINE).strip()
        if not text_prompt:
            text_prompt = "Please analyze the image and solve the exercise."

        if not os.path.exists(image_path):
            # helpful error for debugging
            raise FileNotFoundError(
                f"Image not found at: {image_path}\n"
                "Make sure the path is readable by the server process that runs FastAPI/Ollama. "
                "If Ollama runs in a container, mount the directory into the container."
            )

        # load image and convert to base64 data URL
        pil = Image.open(image_path).convert("RGB")
        buf = BytesIO()
        pil.save(buf, format="JPEG")  # convert to JPEG for smaller size
        img_b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
        data_url = f"data:image/jpeg;base64,{img_b64}"

        image_part = {"type": "image_url", "image_url": data_url}
        text_part = {"type": "text", "text": text_prompt}
        content_parts = [image_part, text_part]

        messages = [SystemMessage(content=final_system_prompt), HumanMessage(content=content_parts)]

        # llm.invoke is synchronous in many installs; run it in a thread not to block the loop
        ai_msg = await asyncio.to_thread(llm.invoke, messages)
        # return ai_msg.content
        end_time = datetime.now()
        response_time_ms = (end_time - start_time).total_seconds() * 1000
        return ai_msg.content, response_time_ms

    else:
        # text-only (safe fallback)
        messages = [("system", final_system_prompt), ("human", user_prompt)]
        ai_msg = await asyncio.to_thread(llm.invoke, messages)
        # return ai_msg.content
        end_time = datetime.now()
        response_time_ms = (end_time - start_time).total_seconds() * 1000
        return ai_msg.content, response_time_ms

# --- Core function ---
async def call_ollama_model(model_name: str, system_prompt: Optional[str], user_prompt: str) -> str:
    try:
        start_time = datetime.now()
        config = MODEL_CONFIGS.get(model_name)
        if not config:
            raise ValueError(f"Model '{model_name}' not found in MODEL_CONFIGS.")

        # Default system prompt if none is given
        final_system_prompt = system_prompt or config["system_prompt"]

        # Init Ollama model
        llm = ChatOllama(
            model=config["name"],
            num_keep=-1,
            seed=42,
            num_predict=-1,
            top_k=20,
            top_p=0.9,
            num_thread=2,
            num_gpu=-1
        )

        # Prompt with system + user content
        template = f"""
        System: {final_system_prompt}

        User: {{input}}
        """

        prompt_template = PromptTemplate(
            input_variables=["input"],
            template=template
        )

        # Run chain
        llm_chain = prompt_template | llm
        response = llm_chain.invoke({"input": user_prompt})
        end_time = datetime.now()
        response_time_ms = (end_time - start_time).total_seconds() * 1000
        return response.content, response_time_ms
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")


# Health check
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Test database connection
        connection = oracledb.connect(user="SYS", password="oracle", dsn="10.42.0.243:1521/FREE", mode=oracledb.SYSDBA)
        cursor = connection.cursor()
        cursor.execute("SELECT 1 FROM DUAL")
        cursor.fetchone()
        
        # Test Ollama connection
        async with httpx.AsyncClient(timeout=300) as client:
            response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
            response.raise_for_status()
        
        return {"status": "healthy", "database": "connected", "ollama": "connected"}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

# Model info endpoint
@app.get("/models")
async def get_models():
    """Get information about available models"""
    return {"models": MODEL_CONFIGS}


import time

@app.post("/generate/{model_key}")
async def generate_response(model_key: str, request: PromptRequest):
    if model_key != "llava":
        if model_key not in MODEL_CONFIGS:
            raise HTTPException(status_code=404, detail="Model not found")

        config = MODEL_CONFIGS[model_key]
        model_name = config["name"]
        system_prompt = MODEL_CONFIGS[f"{model_key}"]["system_prompt"]

        try:
            cpu_usages = []  # store CPU usage samples
            stop_event = asyncio.Event()  # used to signal monitor to stop

            # Monitoring task
            async def monitor_cpu():
                while not stop_event.is_set():
                    cpu_usage = await get_system_metrics()
                    cpu_usages.append(cpu_usage)
                    await asyncio.sleep(0.1)  # sample every 100ms

            # Start monitoring
            monitor_task = asyncio.create_task(monitor_cpu())    
            # Measure time
            start_time = time.time()
            response_text, response_time_ms = await call_ollama_model(
                model_name=f"{model_key}",
                system_prompt=system_prompt,
                user_prompt=f"{request.user_prompt}"
            )
            end_time = time.time()

            # Stop monitoring
            stop_event.set()
            await monitor_task  # wait until it finishes cleanly

            total_time = end_time - start_time

            # Save to DB
            interaction_id = await save_interaction(
                model_name, system_prompt, request.user_prompt,
                response_text, response_time_ms, True
            )

            await save_cpu_usage(
                interaction_id, model_name, request.user_prompt,
                response_time_ms, cpu_usages
            )

            return LLMResponse(
                response=response_text,
                model=model_name,
                prompt_id=interaction_id,
                response_time_ms=response_time_ms
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")
    elif model_key == "llava":
        if model_key not in MODEL_CONFIGS:
            raise HTTPException(status_code=404, detail="Model not found")

        config = MODEL_CONFIGS[model_key]
        model_name = config["name"]
        system_prompt = MODEL_CONFIGS[f"{model_key}"]["system_prompt"]

        try:
            cpu_usages = []  # store CPU usage samples
            stop_event = asyncio.Event()  # used to signal monitor to stop

            # Monitoring task
            async def monitor_cpu():
                while not stop_event.is_set():
                    cpu_usage = await get_system_metrics()
                    cpu_usages.append(cpu_usage)
                    await asyncio.sleep(0.1)  # sample every 100ms

            # Start monitoring
            monitor_task = asyncio.create_task(monitor_cpu())    
            # Measure time
            start_time = time.time()
            response_text, response_time_ms = await call_ollama_model_for_image_to_text(
                model_name=f"{model_key}",
                system_prompt=system_prompt,
                user_prompt=f"{request.user_prompt}"
            )
            end_time = time.time()

            # Stop monitoring
            stop_event.set()
            await monitor_task  # wait until it finishes cleanly

            total_time = end_time - start_time

            # Save to DB
            interaction_id = await save_interaction(
                model_name, system_prompt, request.user_prompt,
                response_text, response_time_ms, True
            )

            await save_cpu_usage(
                interaction_id, model_name, request.user_prompt,
                response_time_ms, cpu_usages
            )

            return LLMResponse(
                response=response_text,
                model=model_name,
                prompt_id=interaction_id,
                response_time_ms=response_time_ms
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")
    
async def save_cpu_usage(interaction_id: int, model_name: str, user_prompt: str,
                       response_time_ms: float, cpu_usages: list):
    """Save CPU usage metrics to the database."""
    try:
        connection = oracledb.connect(user="SYS", password="oracle", dsn="10.42.0.243:1521/FREE", mode=oracledb.SYSDBA)
        cursor = connection.cursor()

        # Prepare the SQL statement
        sql = """
        INSERT INTO cpu_usage_metrics (interaction_id, model_name, user_prompt, 
                                     response_time_ms, cpu_usage, timestamp)
        VALUES (:interaction_id, :model_name, :user_prompt, 
                :response_time_ms, :cpu_usage, SYSTIMESTAMP)
        """

        # Prepare the data for each CPU usage entry
        data = []
        for cpu_usage in cpu_usages:
            data.append({
                'interaction_id': interaction_id,
                'model_name': model_name,
                'user_prompt': user_prompt,
                'response_time_ms': response_time_ms,
                'cpu_usage': cpu_usage
            })

        # Execute the query for each set of data
        cursor.executemany(sql, data)
        connection.commit()

    except Exception as e:
        logger.error(f"Error saving CPU usage metrics to database: {e}")
        raise HTTPException(status_code=500, detail="Failed to save CPU usage metrics to database")

# For C# coding
# POST
# http://0.0.0.0:8000/llamasharp
# Body {"user_prompt": ""}
# Response: response, model, prompt_id, response_time_ms
# Specialized endpoints for each model
@app.post("/llamasharp", response_model=LLMResponse)
async def llamasharp_generate(request: PromptRequest):
    """Generate response using LlamaSharp (reasoning and analysis)"""
    return await generate_response("llamasharp", request)

# For Python/JavaScript coding
# POST
# http://0.0.0.0:8000/codeqwen
# Body {"user_prompt": ""}
# Response: response, model, prompt_id, response_time_ms
# Specialized endpoints for each model
@app.post("/codeqwen", response_model=LLMResponse)
async def codeqwen_generate(request: PromptRequest):
    """Generate code using CodeQwen (code generation)"""
    return await generate_response("codeqwen", request)

# For Python/JavaScript coding
# POST
# http://0.0.0.0:8000/codellama
# Body {"user_prompt": ""}
# Response: response, model, prompt_id, response_time_ms
# Specialized endpoints for each model
@app.post("/codellama", response_model=LLMResponse)
async def codellama_generate(request: PromptRequest):
    """Analyze/debug code using CodeLlama (code analysis)"""
    return await generate_response("codellama", request)

# For conversation
# POST
# http://0.0.0.0:8000/mistral
# Body {"user_prompt": ""}
# Response: response, model, prompt_id, response_time_ms
# Specialized endpoints for each model
@app.post("/mistral", response_model=LLMResponse)
async def mistral_generate(request: PromptRequest):
    """Generate response using Mistral (general purpose)"""
    return await generate_response("mistral", request)

# For conversation
# POST
# http://0.0.0.0:8000/phi3
# Body {"user_prompt": ""}
# Response: response, model, prompt_id, response_time_ms
# Specialized endpoints for each model
@app.post("/phi3", response_model=LLMResponse)
async def phi3_generate(request: PromptRequest):
    """Generate response using Phi-3 (efficient responses)"""
    return await generate_response("phi3", request)

# For conversation
# POST
# http://0.0.0.0:8000/phi3
# Body {"user_prompt": ""}
# Response: response, model, prompt_id, response_time_ms
# Specialized endpoints for each model
@app.post("/llava", response_model=LLMResponse)
async def phi3_generate(request: PromptRequest):
    """Generate response using LLava (efficient responses)"""
    return await generate_response("llava", request)

# History Create 
# http://localhost:8000/interaction/{search_prompt}
# Returns a list with these items:
# id, model_name, system_prompt, user_prompt, response_text, created_at, response_time_ms, success
@app.get("/interaction/{search_prompt}")
async def get_interaction__prompt_search(search_prompt: str):
    """Get a specific interaction by Prompt String"""
    try:
        connection = oracledb.connect(user="SYS", password="oracle", dsn="10.42.0.243:1521/FREE", mode=oracledb.SYSDBA)
        cursor = connection.cursor()
        cursor.execute(f"""
            SELECT id, model_name, system_prompt, user_prompt, 
                    response_text, created_at, response_time_ms, success
            FROM llm_interactions 
            WHERE user_prompt LIKE '%{search_prompt}%'
        """)

        rows = cursor.fetchall()
        interactions = []
        for row in rows:
            interactions.append({
                "id": row[0],
                "model_name": row[1],
                "system_prompt": row[2].read(),
                "user_prompt": row[3].read(),
                "response_text": row[4].read(),
                "created_at": row[5],
                "response_time_ms": row[6],
                "success": bool(row[7])
            })
        return interactions
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


# Database query endpoints
@app.get("/interactions/{interaction_id}")
async def get_interaction(interaction_id: int):
    """Get a specific interaction by ID"""
    try:
        connection = oracledb.connect(user="SYS", password="oracle", dsn="10.42.0.243:1521/FREE", mode=oracledb.SYSDBA)
        cursor = connection.cursor()
        cursor.execute("""
            SELECT id, model_name, system_prompt, user_prompt, 
                    response_text, created_at, response_time_ms, success
            FROM llm_interactions 
            WHERE id = :id
        """, {'id': interaction_id})
        
        row = cursor.fetchone()
        if not row:
            raise HTTPException(status_code=404, detail="Interaction not found")
        
        return {
            "id": row[0],
            "model_name": row[1],
            "system_prompt": row[2],
            "user_prompt": row[3],
            "response_text": row[4],
            "created_at": row[5],
            "response_time_ms": row[6],
            "success": bool(row[7])
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@app.get("/interactions")
async def get_interactions(limit: int = 10, offset: int = 0, model_name: Optional[str] = None):
    """Get list of interactions with pagination"""
    try:
        connection = oracledb.connect(user="SYS", password="oracle", dsn="10.42.0.243:1521/FREE", mode=oracledb.SYSDBA)
        cursor = connection.cursor()
        
        base_query = """
            SELECT id, model_name, user_prompt, response_text, 
                    created_at, response_time_ms, success
            FROM llm_interactions
        """
        
        if model_name:
            base_query += " WHERE model_name = :model_name"
            params = {'model_name': model_name, 'offset': offset, 'limit': limit}
        else:
            params = {'offset': offset, 'limit': limit}
        
        query = base_query + " ORDER BY created_at DESC OFFSET :offset ROWS FETCH NEXT :limit ROWS ONLY"
        
        cursor.execute(query, params)
        rows = cursor.fetchall()
        
        interactions = []
        for row in rows:
            interactions.append({
                "id": row[0],
                "model_name": row[1],
                "user_prompt": row[2][:100] + "..." if len(row[2]) > 100 else row[2],
                "response_preview": row[3][:200] + "..." if len(row[3]) > 200 else row[3],
                "created_at": row[4],
                "response_time_ms": row[5],
                "success": bool(row[6])
            })
        
        return {"interactions": interactions, "limit": limit, "offset": offset}
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)